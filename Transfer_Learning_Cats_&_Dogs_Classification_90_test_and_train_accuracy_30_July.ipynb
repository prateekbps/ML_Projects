{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOHMH0UFCO+SJjQXtEIfmOw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prateekbps/ML_Projects/blob/master/Transfer_Learning_Cats_%26_Dogs_Classification_90_test_and_train_accuracy_30_July.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BooGZa_bsEC6"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d salader/dogs-vs-cats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgQ56IRwQaOy",
        "outputId": "11f382a2-ed10-42b6-d6fc-2a803ba76e8d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/salader/dogs-vs-cats\n",
            "License(s): unknown\n",
            "Downloading dogs-vs-cats.zip to /content\n",
            "100% 1.06G/1.06G [00:50<00:00, 23.1MB/s]\n",
            "100% 1.06G/1.06G [00:50<00:00, 22.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_file = zipfile.ZipFile('/content/dogs-vs-cats.zip','r')\n",
        "zip_file.extractall('/content')\n",
        "zip_file.close()\n"
      ],
      "metadata": {
        "id": "FOEO7fBSQZfO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from keras.models import Sequential"
      ],
      "metadata": {
        "id": "dFfVc6pxQf96"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre trained model\n",
        "from keras.applications.vgg16 import VGG16"
      ],
      "metadata": {
        "id": "W6RiYwG7Qohw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load VGG16 model without the top layers (include_top=False)\n",
        "# Fine tuning model as we are changing the block 5\n",
        "# Fine tuning and feature extraction\n",
        "vgg16_model = VGG16(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(256,256,3),\n",
        ")\n",
        "\n",
        "# Make the model trainable initially\n",
        "vgg16_model.trainable = True\n",
        "\n",
        "# Variable to track when to start setting layers as trainable\n",
        "set_trainable = False\n",
        "\n",
        "# Iterate over the layers of the VGG16 model\n",
        "for layer in vgg16_model.layers:\n",
        "    # Find the layer 'block5_conv1'\n",
        "    if layer.name == 'block5_conv1':\n",
        "        set_trainable = True  # Start setting layers as trainable\n",
        "    if set_trainable:\n",
        "        layer.trainable = True  # Set layers to be trainable\n",
        "    else:\n",
        "        layer.trainable = False  # Set layers before 'block5_conv1' as non-trainable\n",
        "\n",
        "# Print out the name of each layer and whether it is trainable\n",
        "for layer in vgg16_model.layers:\n",
        "    print(layer.name, layer.trainable)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gvbj0Q1WQ-Mc",
        "outputId": "cd086cc9-9fc0-414f-fc05-8e6178ba2bef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "input_layer False\n",
            "block1_conv1 False\n",
            "block1_conv2 False\n",
            "block1_pool False\n",
            "block2_conv1 False\n",
            "block2_conv2 False\n",
            "block2_pool False\n",
            "block3_conv1 False\n",
            "block3_conv2 False\n",
            "block3_conv3 False\n",
            "block3_pool False\n",
            "block4_conv1 False\n",
            "block4_conv2 False\n",
            "block4_conv3 False\n",
            "block4_pool False\n",
            "block5_conv1 True\n",
            "block5_conv2 True\n",
            "block5_conv3 True\n",
            "block5_pool True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(vgg16_model)\n",
        "model.add(Flatten())\n",
        "# change the DNN Part\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "sxjieKvMW4B9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "Xfm0bNQ-YDcJ",
        "outputId": "3c58d182-9c49-431a-8bf1-bc974142b488"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │      \u001b[38;5;34m14,714,688\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32768\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │       \u001b[38;5;34m8,388,864\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m257\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32768</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,388,864</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,103,809\u001b[0m (88.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,103,809</span> (88.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,468,545\u001b[0m (59.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,468,545</span> (59.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,635,264\u001b[0m (29.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,635,264</span> (29.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_model.trainable = False"
      ],
      "metadata": {
        "id": "y6OUihjsYVvg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
      ],
      "metadata": {
        "id": "22sVTBaNY0Eh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=0.2, vertical_flip=True,\n",
        "                                   horizontal_flip=True, zoom_range=0.3, height_shift_range=0.2,\n",
        "                                   shear_range=0.3)\n",
        "\n",
        "test_datagen= train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory('/content/train', target_size = (256,256),\n",
        "                                                    batch_size=64, class_mode = 'binary')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory('/content/test', target_size = (256,256),\n",
        "                                                    batch_size=64, class_mode = 'binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frVaPlYbS8R_",
        "outputId": "e1537fd7-2700-494b-dfba-b90db10d55ea"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 images belonging to 2 classes.\n",
            "Found 5000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
        "history = model.fit(train_generator, validation_data=validation_generator, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K--Cg-HYUuKg",
        "outputId": "3a11caf5-641d-41b9-cbaf-118b1b1d8728"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 557ms/step - accuracy: 0.6896 - loss: 0.9985 - val_accuracy: 0.9298 - val_loss: 0.2359\n",
            "Epoch 2/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 468ms/step - accuracy: 0.9325 - loss: 0.1833 - val_accuracy: 0.9332 - val_loss: 0.1597\n",
            "Epoch 3/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 504ms/step - accuracy: 0.9549 - loss: 0.1192 - val_accuracy: 0.9402 - val_loss: 0.1409\n",
            "Epoch 4/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 468ms/step - accuracy: 0.9680 - loss: 0.0921 - val_accuracy: 0.9398 - val_loss: 0.1402\n",
            "Epoch 5/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 467ms/step - accuracy: 0.9796 - loss: 0.0663 - val_accuracy: 0.9384 - val_loss: 0.1455\n",
            "Epoch 6/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 467ms/step - accuracy: 0.9828 - loss: 0.0559 - val_accuracy: 0.9352 - val_loss: 0.1644\n",
            "Epoch 7/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 464ms/step - accuracy: 0.9869 - loss: 0.0420 - val_accuracy: 0.9366 - val_loss: 0.1566\n",
            "Epoch 8/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 500ms/step - accuracy: 0.9936 - loss: 0.0301 - val_accuracy: 0.9376 - val_loss: 0.1809\n",
            "Epoch 9/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 467ms/step - accuracy: 0.9955 - loss: 0.0232 - val_accuracy: 0.9406 - val_loss: 0.1711\n",
            "Epoch 10/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 467ms/step - accuracy: 0.9980 - loss: 0.0164 - val_accuracy: 0.9382 - val_loss: 0.1831\n"
          ]
        }
      ]
    }
  ]
}